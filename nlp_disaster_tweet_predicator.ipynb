{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cf41b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajmer\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\ajmer\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\ajmer\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\ajmer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115557ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\ajmer\\NLP\\train (2).csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860e67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentences):\n",
    "  tweet_list = []\n",
    "  filter = string.ascii_letters + \" \"\n",
    "  lm = WordNetLemmatizer()\n",
    "  for tweet in sentences:\n",
    "    tweet_cleaned = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet_cleaned = \"\".join([chr for chr in tweet_cleaned if chr in filter])\n",
    "    tweet_cleaned = tweet_cleaned.lower()\n",
    "    tokens = word_tokenize(tweet_cleaned)\n",
    "    clean_list = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    clean_list = [lm.lemmatize(word) for word in clean_list]\n",
    "    tweet_cleaned = \" \".join(clean_list)\n",
    "    tweet_list.append(tweet_cleaned)\n",
    "\n",
    "  return np.array(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa01b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=preprocess(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1060387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN         deed reason earthquake may allah forgive u   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  resident asked shelter place notified officer ...   \n",
       "3   6     NaN      NaN  people receive wildfire evacuation order calif...   \n",
       "4   7     NaN      NaN  got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61bf4828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df[\"text\"].duplicated().sum()\n",
    "df.drop_duplicates(subset=[\"text\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f4dd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d160c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Use train_test_split to split training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61867ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5479, 5479, 1370, 1370)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(X_train), len(y_train), len(X_valid), len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7533d6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4297                              hellfireev jackperu one\n",
       "4422    ransomware hold bc man computer file hostage v...\n",
       "4283    day without phone due slow shite computer utte...\n",
       "275     rt janenelson rt stephenscifi adaptation watch...\n",
       "876     rolandonabeats ellie goulding blood acesse nos...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2b81c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13578"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique words in X_train\n",
    "vocab = []\n",
    "for tweet in X_train:\n",
    "    vocab.extend(tweet.split())\n",
    "len(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "082d0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "text_vectorizer = TextVectorization(max_tokens=None, \n",
    "                                    standardize=\"lower_and_strip_punctuation\", \n",
    "                                    split=\"whitespace\", \n",
    "                                    ngrams=None, \n",
    "                                    output_mode=\"int\", \n",
    "                                    output_sequence_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9d594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b68fe27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1515"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(range(len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fabb959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening blower tuffers aussie batting collapse trent bridge reminds love bbctms wonderful stuff engvaus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(14,), dtype=int64, numpy=\n",
       "array([ 1362, 12707,  5486,  2370,  4636,    78,  1173,   278,  1498,\n",
       "          34, 12920,  1931,   972, 11075], dtype=int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_index=random.choice(range(len(X_train)))\n",
    "tweet=X_train.iloc[random_index]\n",
    "print(tweet)\n",
    "text_vectorizer(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b34ab80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer(\"hjgfhjdg djshflihlkjdsaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebcaef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13580"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_in_vocab=text_vectorizer.get_vocabulary()\n",
    "len(word_in_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6041223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexhammerstone',\n",
       " 'alexeivolkov',\n",
       " 'alexandrapullin',\n",
       " 'alekalicante',\n",
       " 'aleisstokes',\n",
       " 'alec',\n",
       " 'aldridge',\n",
       " 'alcoholismaddiction',\n",
       " 'alcoholandmetal',\n",
       " 'albertsons',\n",
       " 'albertans',\n",
       " 'albeit',\n",
       " 'albany',\n",
       " 'alba',\n",
       " 'alaskan',\n",
       " 'alarmingly',\n",
       " 'alarming',\n",
       " 'alarmems',\n",
       " 'alamodc',\n",
       " 'alabamaquake',\n",
       " 'ala',\n",
       " 'akxbskdn',\n",
       " 'akx',\n",
       " 'akwa',\n",
       " 'akumareisu',\n",
       " 'akrams',\n",
       " 'akito',\n",
       " 'akilah',\n",
       " 'akgovbillwalker',\n",
       " 'akame',\n",
       " 'ajw',\n",
       " 'ajabrown',\n",
       " 'aitchkaycee',\n",
       " 'aisumage',\n",
       " 'aisle',\n",
       " 'airwave',\n",
       " 'airstrikes',\n",
       " 'airlift',\n",
       " 'airing',\n",
       " 'airi',\n",
       " 'airhorns',\n",
       " 'airhead',\n",
       " 'airbullet',\n",
       " 'airasia',\n",
       " 'aintsheperty',\n",
       " 'aimlessly',\n",
       " 'aiii',\n",
       " 'aiginsurance',\n",
       " 'aidan',\n",
       " 'aias',\n",
       " 'ahuh',\n",
       " 'ahrar',\n",
       " 'ahmazing',\n",
       " 'ahhtheenikki',\n",
       " 'ahhhhh',\n",
       " 'ahhhh',\n",
       " 'ahahahga',\n",
       " 'agusa',\n",
       " 'aguero',\n",
       " 'agtparis',\n",
       " 'agthanover',\n",
       " 'agrees',\n",
       " 'agreement',\n",
       " 'agochicago',\n",
       " 'agnus',\n",
       " 'aggressively',\n",
       " 'aggressif',\n",
       " 'aggarwal',\n",
       " 'ageekyfangirl',\n",
       " 'agdq',\n",
       " 'agalloch',\n",
       " 'afycso',\n",
       " 'aftershockorg',\n",
       " 'aftershockdelo',\n",
       " 'afterhours',\n",
       " 'afterhaiyan',\n",
       " 'afte',\n",
       " 'afrin',\n",
       " 'africansinsf',\n",
       " 'afp',\n",
       " 'afloat',\n",
       " 'afk',\n",
       " 'afghetcleft',\n",
       " 'affecting',\n",
       " 'aesthetic',\n",
       " 'aerospace',\n",
       " 'aeroplane',\n",
       " 'aelinrhee',\n",
       " 'aeg',\n",
       " 'adweek',\n",
       " 'advice',\n",
       " 'advertised',\n",
       " 'advertise',\n",
       " 'adverse',\n",
       " 'advantage',\n",
       " 'adumbbb',\n",
       " 'adriennetomah',\n",
       " 'adrianpeel',\n",
       " 'adorableappple',\n",
       " 'adorable',\n",
       " 'adoptive',\n",
       " 'adoption',\n",
       " 'adopt',\n",
       " 'adndotcom',\n",
       " 'adjuster',\n",
       " 'adjustable',\n",
       " 'adjust',\n",
       " 'adiossuperbacterias',\n",
       " 'adidas',\n",
       " 'adelaide',\n",
       " 'addition',\n",
       " 'addiction',\n",
       " 'addict',\n",
       " 'adaptation',\n",
       " 'adanne',\n",
       " 'adani',\n",
       " 'adamtuss',\n",
       " 'adamrubinespn',\n",
       " 'adamnibloe',\n",
       " 'adamantly',\n",
       " 'acura',\n",
       " 'actor',\n",
       " 'activist',\n",
       " 'activision',\n",
       " 'actively',\n",
       " 'activate',\n",
       " 'actionmoviestaughtus',\n",
       " 'acted',\n",
       " 'actavis',\n",
       " 'acrylic',\n",
       " 'acronym',\n",
       " 'acquired',\n",
       " 'acquiesce',\n",
       " 'acousticmaloley',\n",
       " 'acoustic',\n",
       " 'acne',\n",
       " 'aching',\n",
       " 'achimota',\n",
       " 'achieving',\n",
       " 'achievement',\n",
       " 'achieve',\n",
       " 'achedin',\n",
       " 'acesse',\n",
       " 'acenewsdesk',\n",
       " 'acebreakingnews',\n",
       " 'acebabes',\n",
       " 'ace',\n",
       " 'acdelco',\n",
       " 'acdcd',\n",
       " 'accustomed',\n",
       " 'accuses',\n",
       " 'accuracy',\n",
       " 'accountable',\n",
       " 'accordingly',\n",
       " 'accidentwho',\n",
       " 'accidently',\n",
       " 'accidentalprophecy',\n",
       " 'accepts',\n",
       " 'accept',\n",
       " 'acarewornheart',\n",
       " 'abysmaljoiner',\n",
       " 'abuseddesolateamplost',\n",
       " 'abused',\n",
       " 'abubaraa',\n",
       " 'absurd',\n",
       " 'abstract',\n",
       " 'absolutsumya',\n",
       " 'absence',\n",
       " 'abovewould',\n",
       " 'abouts',\n",
       " 'abombed',\n",
       " 'abomb',\n",
       " 'aboard',\n",
       " 'abninfvet',\n",
       " 'ableg',\n",
       " 'abject',\n",
       " 'abia',\n",
       " 'abha',\n",
       " 'abetter',\n",
       " 'abes',\n",
       " 'aberystwythshrewsbury',\n",
       " 'aberdeenfc',\n",
       " 'aberdeenfanpage',\n",
       " 'aberdeen',\n",
       " 'abcnorio',\n",
       " 'abceyewitness',\n",
       " 'abcchicago',\n",
       " 'abbyairshow',\n",
       " 'abbandoned',\n",
       " 'abandoning',\n",
       " 'abandonedpics',\n",
       " 'aashiqui',\n",
       " 'aaronthefm',\n",
       " 'aannnnd',\n",
       " 'aan',\n",
       " 'aampw',\n",
       " 'aal',\n",
       " 'aaceorg',\n",
       " 'aaaaaaallll',\n",
       " 'aaaa']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_in_vocab[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5030ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
